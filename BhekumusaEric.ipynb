{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhekumusaEric/1callrr_website/blob/main/BhekumusaEric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9pJrIFM1NhB"
      },
      "source": [
        "# Welcome to your META-Powered PDF Question & Answer Assistant  \n",
        "*Build a simple RAG system in Google Colab using Meta Llama models*\n",
        "\n",
        "This notebook will help you build an AI assistant that can read any PDF you give it and answer questions **only** based on that PDF.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Load a PDF (e.g. report, policy, curriculum, research paper)\n",
        "- Ask natural questions about it\n",
        "- Get short, accurate answers grounded in the document\n",
        "\n",
        "The AI uses **Retrieval-Augmented Generation (RAG)**: it searches the PDF first, then answers using only what it finds (no guessing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQTr7DhI1NhK"
      },
      "source": [
        "---\n",
        "\n",
        "## 0. Prerequisites\n",
        "\n",
        "Before you start, make sure you have:\n",
        "\n",
        "1. **Google Colab access**  \n",
        "   You will run all code in a Colab notebook.\n",
        "\n",
        "2. **OpenRouter API key**  \n",
        "   Follow this video to get a key:  \n",
        "   https://www.youtube.com/watch?v=-X9DVzzxpAA\n",
        "\n",
        "3. **A Google Drive link to your PDF**  \n",
        "   Any PDF stored in Google Drive (set to \"Anyone with the link\" or at least accessible to your account)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFL78nog1NhM"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Install all required libraries (Step 1)\n",
        "\n",
        "In this step, you install all the tools your AI assistant needs.  \n",
        "Run this cell **once per Colab session**.\n",
        "\n",
        "**What you install:**\n",
        "\n",
        "- `llama-index` â€“ framework for reading, chunking, indexing and querying documents\n",
        "- `llama-index-llms-openrouter` â€“ connects to Meta Llama models via OpenRouter\n",
        "- `llama-index-embeddings-huggingface` â€“ creates embeddings for semantic search\n",
        "- `llama-index-readers-file` â€“ reads PDFs and other files\n",
        "- `llama-index-packs-fusion-retriever` â€“ Meta \"Query Fusion\" retriever pack\n",
        "- `sentence-transformers` â€“ semantic understanding and chunking\n",
        "- `nest-asyncio` â€“ fixes async issues in Colab\n",
        "- `requests` â€“ downloads the PDF from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RuczfFAI1NhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab2775c-d260-4b3a-e3e6-72d9893f2c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Installation complete\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "  llama-index \\\n",
        "  llama-index-llms-openrouter \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-packs-fusion-retriever \\\n",
        "  sentence-transformers \\\n",
        "  nest-asyncio \\\n",
        "  requests\n",
        "\n",
        "print(\"âœ… Installation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PCm9yxP1NhS"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Connect to the AI model (Step 2)\n",
        "\n",
        "Here you:\n",
        "\n",
        "- Import core libraries\n",
        "- Enter your **OpenRouter API key**\n",
        "- Configure the **Llama model**\n",
        "- Configure the **embedding model**\n",
        "- Tell `llama-index` to use them\n",
        "\n",
        "Run this cell **after** Step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UYSZtJu01NhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703cda80-1bc1-483f-c30e-b6e14f61a200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… AI model and settings are ready to use\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Ask for your OpenRouter API key (input is hidden like a password)\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter API key: \")\n",
        "\n",
        "# Configure the LLM (Meta Llama via OpenRouter)\n",
        "llm = OpenRouter(\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "    max_tokens=512,\n",
        "    temperature=0.1,  # Low = more precise, less \"creative\"\n",
        "    timeout=60,\n",
        "    system_prompt=(\n",
        "        \"You are an expert RAG system that answers ONLY using the provided context. \"\n",
        "        \"Never hallucinate. Never guess. If the answer is not in the context, say so. \"\n",
        "        \"Provide short, clear, factual responses with 2â€“4 evidence bullets.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Configure the embedding model\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Register both with LlamaIndex settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print(\"âœ… AI model and settings are ready to use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BK4frCW1Nhd"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Download the PDF from Google Drive (Step 3)\n",
        "\n",
        "This step:\n",
        "\n",
        "1. Asks you for a **Google Drive link** to your PDF\n",
        "2. Extracts the **file ID** from the link\n",
        "3. Downloads the PDF into a local `data/` folder\n",
        "4. Saves it as `data/source.pdf`\n",
        "\n",
        "Supported link formats include:\n",
        "\n",
        "- `https://drive.google.com/file/d/<FILE_ID>/view?...`\n",
        "- `https://drive.google.com/open?id=<FILE_ID>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XT7L8jv1Nhe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
        "    \"\"\"\n",
        "    Download a PDF from a Google Drive sharing link and save it locally.\n",
        "    \"\"\"\n",
        "    # Try pattern: /d/<FILE_ID>/\n",
        "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "    else:\n",
        "        # Try pattern: ?id=<FILE_ID>\n",
        "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"âŒ Could not extract file ID from the link.\")\n",
        "\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    print(f\"ğŸ“¥ Downloading PDF (file ID {file_id})...\")\n",
        "\n",
        "    resp = requests.get(download_url)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "\n",
        "    print(f\"âœ… PDF downloaded â†’ {save_path}\")\n",
        "\n",
        "# Ask for the Drive link\n",
        "drive_link = input(\"ğŸ“Œ Paste your Google Drive PDF link here: \").strip()\n",
        "\n",
        "# Make sure the data folder exists\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Local path for the PDF\n",
        "pdf_path = os.path.join(DATA_DIR, \"source.pdf\")\n",
        "\n",
        "# Download the PDF\n",
        "download_pdf_from_drive(drive_link, pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryC1Nax1Nhi"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Break the PDF into semantic chunks (Step 4)\n",
        "\n",
        "The AI cannot use one giant block of text.  \n",
        "Here you:\n",
        "\n",
        "- Load the PDF\n",
        "- Use a **semantic splitter** to create \"smart\" chunks (not random splits)\n",
        "- Label each chunk with simple metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU8P9L6A1Nhm"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Load the PDF as a document\n",
        "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
        "print(f\"ğŸ“„ Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# Embedding model for semantic splitting (can reuse the same model name)\n",
        "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Create a semantic splitter\n",
        "parser = SemanticSplitterNodeParser(\n",
        "    buffer_size=3,\n",
        "    breakpoint_percentile_threshold=95,\n",
        "    embed_model=semantic_embed,\n",
        ")\n",
        "\n",
        "# Generate semantic nodes (chunks)\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Add simple metadata to each chunk\n",
        "for n in nodes:\n",
        "    n.metadata[\"source\"] = pdf_path\n",
        "    n.metadata[\"chunk_type\"] = \"semantic\"\n",
        "\n",
        "print(f\"ğŸ” Created {len(nodes)} high-quality semantic nodes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyfHUHt51Nhm"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Build the Query Fusion retriever (Step 5)\n",
        "\n",
        "Now you build the **search engine** that powers your RAG system.  \n",
        "It uses **Query Fusion**:\n",
        "\n",
        "- Rewrites your question several ways\n",
        "- Searches multiple times\n",
        "- Fuses the best results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sqd_Lng1Nhn"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "# Download or load the Query Fusion pack\n",
        "QueryRewritingRetrieverPack = download_llama_pack(\n",
        "    \"QueryRewritingRetrieverPack\",\n",
        "    \"./query_rewriting_pack\",\n",
        ")\n",
        "\n",
        "# Create the advanced retriever using your nodes\n",
        "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
        "    nodes,                      # semantic chunks from Step 4\n",
        "    chunk_size=256,\n",
        "    vector_similarity_top_k=8,\n",
        "    fusion_similarity_top_k=8,\n",
        "    num_queries=6,              # number of query rewrites\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Advanced Query Fusion RAG Engine Ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBNvOkwq1Nho"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Ask questions in an interactive loop (Step 6)\n",
        "\n",
        "Finally, you create a simple chat loop:\n",
        "\n",
        "- Type a question about the PDF\n",
        "- The system runs the RAG pipeline\n",
        "- You see a clear answer\n",
        "- Type `end` to exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ5I86O01Nho",
        "outputId": "6ff443c7-0718-4832-b87a-74d71f8582b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RAG Interactive Mode\n",
            "Ask any question about your PDF.\n",
            "Type 'end' to exit.\n",
            "\n",
            "\n",
            "ğŸ” Retrieving answer...\n",
            "\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (1/3)...\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (2/3)...\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (3/3)...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION:\n",
            "wht is this about\n",
            "\n",
            "ğŸ§  ANSWER:\n",
            "âŒ Could not generate a valid answer after retries.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "\n",
            "ğŸ” Retrieving answer...\n",
            "\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (1/3)...\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (2/3)...\n",
            "âš ï¸ Error: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}\n",
            "ğŸ” Retrying (3/3)...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION:\n",
            "what is the document about ?\n",
            "\n",
            "ğŸ§  ANSWER:\n",
            "âŒ Could not generate a valid answer after retries.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "\n",
            "ğŸ‘‹ Session ended.\n"
          ]
        }
      ],
      "source": [
        "def safe_rag_run(question, retries=3):\n",
        "    \"\"\"\n",
        "    Run the RAG pipeline with basic retry logic.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            resp = query_rewriting_pack.run(question)\n",
        "\n",
        "            if resp is None or str(resp).strip() == \"\":\n",
        "                raise ValueError(\"Empty LLM response.\")\n",
        "\n",
        "            return resp\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error: {e}\")\n",
        "            print(f\"ğŸ” Retrying ({attempt+1}/{retries})...\")\n",
        "\n",
        "    return \"âŒ Could not generate a valid answer after retries.\"\n",
        "\n",
        "print(\"\\nRAG Interactive Mode\")\n",
        "print(\"Ask any question about your PDF.\")\n",
        "print(\"Type 'end' to exit.\\n\")\n",
        "\n",
        "# Interactive Q&A loop\n",
        "while True:\n",
        "    user_question = input(\"ğŸŸ¦ Enter your question: \").strip()\n",
        "\n",
        "    if user_question.lower() == \"end\":\n",
        "        print(\"\\nğŸ‘‹ Session ended.\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nğŸ” Retrieving answer...\\n\")\n",
        "\n",
        "    # Run the question through the RAG pipeline\n",
        "    response = safe_rag_run(user_question)\n",
        "\n",
        "    print(\"\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "    print(\"â“ QUESTION:\")\n",
        "    print(user_question)\n",
        "    print(\"\\nğŸ§  ANSWER:\")\n",
        "    print(response)\n",
        "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyeM_kUA1Nhp"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Example questions you can try\n",
        "\n",
        "Once everything is running, try questions like:\n",
        "\n",
        "- \"What are the main goals in this document?\"\n",
        "- \"What does this policy say about attendance?\"\n",
        "- \"Summarise the key points in chapter one.\"\n",
        "- \"List all the responsibilities of students mentioned in this document.\"\n",
        "- \"How is assessment described in this curriculum?\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}